{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "809229e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт бибилиотек\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cd2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Парсинг данных с сайта \"ВашКонтроль\"\n",
    "#примерный алгоритм парсинга данных с сайта \"ВашКонтроль\", здесь парситсся часть данных, которая была использована в финальном дата сете. \n",
    "url = 'https://vashkontrol.ru/reviews/'\n",
    "firstpage = 2184000 #опытным путем выяснено, что это значение соответствует отзывам, оставленным в январе 2022 года\n",
    "lastpage = firstpage + 10\n",
    "\n",
    "#функция перебора страниц\n",
    "def get_urls():\n",
    "    listUrl = []\n",
    "    for page in range(firstpage,lastpage):\n",
    "        pageUrl = '{}{}'.format(url,page)\n",
    "        listUrl.append(pageUrl)\n",
    "        #pageId.append(page)\n",
    "    return listUrl\n",
    "len(get_urls())\n",
    "\n",
    "servList =[]\n",
    "commentsList = []\n",
    "authorityList = []\n",
    "dateList= []\n",
    "meanScoreList = []\n",
    "pageId = []\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    for pageData in get_urls():\n",
    "        fullData = requests.get(pageData) #если есть проблемы с сертификатами можно поставить verify = False, хоть это и не рекомендуется)\n",
    "        if fullData.status_code == 200:\n",
    "            soup = BeautifulSoup(fullData.text, 'html.parser')\n",
    "            \n",
    "            #костыль с проверкой отсутствия отзыва при работающей странице\n",
    "            if 'Отзыв с такими реквизитами не существует' not in str(soup.find('div', class_=\"block review\")):\n",
    "                \n",
    "                #парсинг комментария\n",
    "                comments = soup.findAll('div',class_= 'block review-comment pull-left span45')\n",
    "                for comment in range(len(comments)):\n",
    "                    if comments[comment].find('span',class_ = 'title') is not None:\n",
    "                        commentsList.append(comments[comment].text.split(':')[1])\n",
    "\n",
    "                #парсинг названия услуги\n",
    "                texts = soup.findAll('div',class_= 'text')\n",
    "                name = str(texts[0]).split(texts[0].findChild(\"a\")['href'])[1].replace('</a></div>','').replace('\">','')\n",
    "                servList.append(name)\n",
    "\n",
    "                #парсинг ведомства предоставляющего услугу\n",
    "                #authorities = soup.findAll('div',class_= 'text')\n",
    "                authority = str(texts[3]).split(texts[3].findChild(\"a\")['href'])[1].replace('</a></div>','').replace('\">','')\n",
    "                authorityList.append(authority)\n",
    "\n",
    "                #парсинг даты оставления отзыва\n",
    "                #authorities = soup.findAll('div',class_= 'text')\n",
    "                date = str(texts[-1].find('span')).replace('<span>','').replace('</span>','')\n",
    "                dateList.append(date)\n",
    "\n",
    "                #парсинг оценок\n",
    "                mList = []\n",
    "                scores = soup.findAll('div',class_= 'scale')\n",
    "                #расчет средней оценки\n",
    "                for score in scores:\n",
    "                    mList.append(int(score['data-value']))\n",
    "                meanScoreList.append(round(np.array(mList).mean(),1))\n",
    "\n",
    "                #добавление страницы в список считанных\n",
    "                pageId.append(pageData.split('reviews/')[1])\n",
    "        time.sleep(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8175da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#вызов функции парсинга данных\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f82f41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание DataFrame из распарсенных данных\n",
    "df_raw = pd.DataFrame(data = {'review_id':pageId, 'date':dateList, 'authority': authorityList, 'serv_name':servList,\n",
    "                         'comment':commentsList, 'mscore':meanScoreList})\n",
    "#предобработка текста\n",
    "df_raw = df_raw[df_raw['comment'].notna()]\n",
    "df_raw['comment'] = df_raw['comment'].str.replace('\\n','')\n",
    "df_raw.replace('None',np.nan, inplace=True)\n",
    "\n",
    "#экспорт информации в csv\n",
    "file_path = os.path.join('..', 'data', 'reviews.csv')#путь для выгрузки данных\n",
    "df_raw.to_csv(file_path,index=False, na_rep=np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
